{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using deep learning to detect early signs of cognitive disease\n",
    "\n",
    "## Abstract\n",
    "[TODO: write abstract once all the other sections are finished]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Hand drawings require a varying degree of dexterity and cognitive ability, and have historically been used to assess the state of cognitive diseases (Brantjes and Bouma, 1991). The drawings can also be designed to stress a specific motor or cognitive skill; for example, drawings containing small details will require precision whereas drawings containing 3-dimensional representations will require the ability to perceive and reproduce perspective (Clare, 1983).\n",
    "\n",
    "Previous research (Rouleau et al., 1992) has shown great potential in diagnosing Alzheimer's Disease (AD) from patient drawings, mainly focused on clock drawings (Wolf-Klein et al., 1989) which require a combination of motor and cognitive skills (Agrell and Dehlin, 1998); however, more recent work has focused on analysis of data produced by costly medical equipment like brain imaging, typically via magnetic resonance imaging (Khedher et al., 2015) but also magnetic resonance tomography (Almkvist and Winblad, 1999). Less invasive tests using biomarkers from blood tests have been studied, but there's still a need for non-invasive, inexpensive tests (Jellinger et al., 2008)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "#### Glossary\n",
    "- **Drawing template**: model given to subjects used as a reference for the hand-drawn sketch.\n",
    "- **Subject drawing**: hand-drawn sketch performed by the subjects attempting to replicate a given drawing template.\n",
    "- **Image scan**: image containing a subject drawing and a drawing template.\n",
    "- **Drawing category**: instance of a drawing template type, for example a house.\n",
    "\n",
    "#### Data collection\n",
    "[TODO: talk about data collection, consent, etc.]\n",
    "\n",
    "<img src=\"../images/drawing_templates.png\">\n",
    "<p align=\"center\" style=\"text-align: center;\">Figure 1. Sample of drawing templates.</p>\n",
    "\n",
    "#### Characteristics\n",
    "The dataset consists of image scans and cognitive evaluations. During the preprocessing stage, image scans with a missing evaluation, subject drawing or very poor image quality are discarded. The resulting dataset used for training contains 3960 subject drawings, roughly following a uniform distribution by drawing category:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Count</th>\n",
    "      <th>Percent</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Drawing Category</th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>casa</th>\n",
    "      <td>416</td>\n",
    "      <td>10.51%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>circulo</th>\n",
    "      <td>441</td>\n",
    "      <td>11.14%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>minimental</th>\n",
    "      <td>436</td>\n",
    "      <td>11.01%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>pico</th>\n",
    "      <td>447</td>\n",
    "      <td>11.29%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>cruz</th>\n",
    "      <td>440</td>\n",
    "      <td>11.11%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>muelle</th>\n",
    "      <td>449</td>\n",
    "      <td>11.34%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>cubo</th>\n",
    "      <td>435</td>\n",
    "      <td>10.98%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>cuadrado</th>\n",
    "      <td>447</td>\n",
    "      <td>11.29%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>triangulo</th>\n",
    "      <td>449</td>\n",
    "      <td>11.34%</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "<p align=\"center\" style=\"text-align: center;\">Figure 2. Drawing category breakdown.</p>\n",
    "\n",
    "Each evaluation results in a coded **diagnosis**. The diagnosis can be one of the following values:\n",
    "- **SANO**: Subject is deemed to be healthy\n",
    "- **DCLNA**: [TODO: describe]\n",
    "- **DCLM**: [TODO: describe]\n",
    "- **DCLA**: [TODO: descibe]\n",
    "- **BAJA**: Low-impact cognitive disease detected\n",
    "- **BAJA EA**: [TODO: describe]\n",
    "- **NO EXISTE**: [TODO: describe]\n",
    "\n",
    "The diagnosis distribution suffers from class imbalance, as shown in the following summary table:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Count</th>\n",
    "      <th>Percent</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Diagnosis</th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>SANO</th>\n",
    "      <td>1984</td>\n",
    "      <td>50.10%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>DCLNA</th>\n",
    "      <td>1017</td>\n",
    "      <td>25.68%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>DCLM</th>\n",
    "      <td>851</td>\n",
    "      <td>21.49%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>DCLA</th>\n",
    "      <td>56</td>\n",
    "      <td>1.41%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>BAJA</th>\n",
    "      <td>33</td>\n",
    "      <td>0.83%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>BAJA EA</th>\n",
    "      <td>9</td>\n",
    "      <td>0.23%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>NO EXISTE</th>\n",
    "      <td>10</td>\n",
    "      <td>0.25%</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "<p align=\"center\" style=\"text-align: center;\">Figure 3. Diagnosis breakdown.</p>\n",
    "\n",
    "However, for the purpose of the analysis, the diagnosis label is simplified into two categories:\n",
    "1. **Healthy**: Equivalent to *SANO*\n",
    "1. **Not healthy**: Any other diagnosis\n",
    "\n",
    "After that transformation, the resulting dataset contains a roughly 50/50 split for the diagnosis label. In this paper, we build a model able to predict whether a subject will have a *not healthy* label based on the subject drawings alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "#### State of the Art\n",
    "Deep neural networks with several convolutional layers currently produce the best results for classification tasks related to hand-srawn sketches (Sert and BoyacÄ±, 2019) as well as more realistic paintings (Shen et al., 2019) or even digital sketches (Ha and Eck, 2018). Several specific deep neural network architectures are commonly used, for example *GoogLeNet* was recently used to perform computer-aided diagnoses from medical scans (Balagourouchetty et al., 2019) and *ResNet34* for assessment of breast tumor cellularity (Rakhlin et al., 2019).\n",
    "\n",
    "#### Preprocessing\n",
    "Data preprocessing was a crucial component of the learning pipeline due to the following challenging characteristics of the dataset:\n",
    "- The dataset is relatively small for the proper application of deep learning techniques (Soekhoe et al., 2016).\n",
    "- Each image scan contains both the subject drawings and the drawing templates, presenting a non-trivial challenge to separate them during processing. Further, the placement of the subject drawings and drawing templates was inconsistent across image scans.\n",
    "- The image scans frequently contain a lot of noise, and the images were not scanned using consistent parameters which lead to image attributes like white balance being observably different across different scans.\n",
    "- The medium used, pencil and paper, leads to varying degrees of contrast in the subject drawing; some subjects pressed the pencil a lot harder against the paper than others. \n",
    "- Some image scans contain multiple subject drawings, which required for some criteria to be established in order to consistently choose which one to evaluate.\n",
    "\n",
    "<img src=\"../images/preprocessing_issues.png\">\n",
    "<p align=\"center\" style=\"text-align: center;\">Figure 4. Preprocessing issues.</p>\n",
    "\n",
    "The following steps were taken in order to preprocess the dataset:\n",
    "1. One image scan for each drawing template was hand-picked and the drawing template was cropped using the open source image editing software *GIMP*.\n",
    "1. Using purpose-built software, the coordinates of the subject drawing and the template were extracted from the image scans. If more than one subject drawing was present, the subject drawing that most resembled the template was chosen after a visual comparison.\n",
    "1. Using the coordinates as an approximate reference, the subject drawing was extracted from the image scan, and subsequently denoised and binarized using well-established computer vision techniques (Szeliski, 2011) such as non-local means denoising (Buades et al., 2011) and adaptive thresholding.\n",
    "\n",
    "<img src=\"../images/preprocessed_drawing.png\">\n",
    "<p align=\"center\" style=\"text-align: center;\">Figure 5. Preprocessed drawing.</p>\n",
    "\n",
    "#### Data Preparation\n",
    "Two independent datasets were used to train different models. For one, drawing-evaluation pairs were used for the input and label, respectively (*ungrouped* dataset). For the other, image drawings were grouped by evaluation, such that each evaluation for a given subject represents a datapoint consisting of N drawings -- one for each template (*grouped* dataset).\n",
    "\n",
    "For the grouped dataset, datapoints where not all drawings are available were discarded, and the evaluation results were encoded into a binary variable indicating whether the subject displayed any cognitive deficiency, or if it was considered healthy.\n",
    "\n",
    "The data is divided into the following standard (ReitermanovÃ¡, 2010) subsets:\n",
    "1. **Train**: datapoints that are fed into the model to compute the error gradient at each iteration.\n",
    "1. **Test**: datapoints used to compute a score at each iteration, which is then used to determine if early stopping is necessary.\n",
    "1. **Validation**: left-out subset used to evaluate the model after training has completed.\n",
    "\n",
    "Subsets are sampled randomly. If the validation subset has a class balance which differs from the class balance of the whole dataset by more than 5%, the subsets are randomly resampled until this criteria is reached. Because of the limited size of the dataset, this prevents cases where a model seemingly performed very well, but in reality the validation subset had a severe problem of class imbalance (Japkowicz and Stephen, 2002).\n",
    "\n",
    "#### Model Training\n",
    "All tried model architectures can be categorized as deep neural networks. Some of the models are an exact copy of known models such as *GoogLeNet* or *ResNet34* (He et al., 2015), known for performing well against the ImageNet dataset and adaptiveness to transfer learning (Talo et al., 2019) including applicability to sketch drawings (Ballester and Araujo, 2016). For those models, the input images were resized to match what the model expects (224x224 in most cases). The models prefixed with *QD-* are heavily inspired by the architecture described by Ha and Eck (2018) applied to the *Quick, Draw!* dataset named *sketch-rnn* (see fig. 11). A number of hyperparameters are identified, such as batch size, ratio of train-to-test and test-to-validation subset size; all combinations of parameter sets are explored using an exhaustive approach. The potential values for the parameters were chosen based on other, well-performing models in the computer vision domain (Russakovsky et al., 2015). Most of the parameters had little effect on the model performance outside of fine-tuning, although bigger kernel and batch sizes yield marginally improved accuracy.\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr style=\"text-align: right;\">\n",
    "            <th>Training Parameter</th>\n",
    "            <th>Values</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr><td>Kernel Size</td><td>3, 5, 7, 9, 11</td></tr>\n",
    "        <tr><td>Batch Size</td><td>24, 32, 48, 56, 64</td></tr>\n",
    "        <tr><td>Test Subset</td><td>20%, 25%</td></tr>\n",
    "        <tr><td>Validation Subset</td><td>20%, 25%</td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p align=\"center\" style=\"text-align: center;\">Figure 6. Summary of training parameters.</p>\n",
    "\n",
    "Removing some drawing categories was also attempted, but due to the high computational overhead required this was not explored exhaustively. In all cases, excluding one or more drawing categories resulted in a statistically significant (Î± > .05 for validation dataset prediction accuracy) worse model performance.\n",
    "\n",
    "During training, the images corresponding to each datapoint are altered using the *imgaug* library (Jung et al., 2019) with the intent to compensate for the total size of the dataset. For the grouped dataset, the pixels of all images in each datapoint are grouped into a single tensor which is used as input for the model.\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr style=\"text-align: right;\">\n",
    "            <th>Image Aumentation Parameter</th>\n",
    "            <th>Values</th>\n",
    "        </tr>\n",
    "    <tbody>\n",
    "        <tr><td>Cropping</td><td>[0%, 10%]</td></tr>\n",
    "        <tr><td>Scaling</td><td>[90%, 110%]</td></tr>\n",
    "        <tr><td>Translation</td><td>[-10%, 10%]</td></tr>\n",
    "        <tr><td>Rotation</td><td>[-10%, 10%]</td></tr>\n",
    "        <tr><td>Shearing</td><td>[-5%, 5%]</td></tr>\n",
    "        <tr><td>Gaussian Blur (Standard Deviation)</td><td>[0, .5]</td></tr>\n",
    "        <tr><td>Pixel Scalar Multiplication</td><td>[.9, 1.1]</td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p align=\"center\" style=\"text-align: center;\">Figure 7. Summary of image augmentation parameters.</p>\n",
    "\n",
    "The following models were trained using nearly identical methodology, other than image grouping for the grouped dataset:\n",
    "- **QD-Grouped**: model trained using the dataset grouped by evaluation.\n",
    "- **QD-Ungrouped**: model trained using the dataset consisting of drawing-evaluation pairs.\n",
    "- **QD-Transfer**: model pretrained using pairs of image-label from the *Quick Draw!* dataset.\n",
    "- **GoogLeNet-Transfer**: model pretrained using *GoogLeNet* architecture with the *ImageNet* dataset.\n",
    "- **ResNet34-Transfer**: model pretrained using *ResNet-34* architecture with the *ImageNet* dataset.\n",
    "\n",
    "For each parameter set, training is performed for 1000 iterations on batches of data (mini-batches) or until the early stopping criteria is reached -- when the accuracy score of the test subset stops increasing after 10 consecutive mini-batches. This process is repeated 3 times, and the average results are reported. This methodology was considered near-optimal after running 3, 5 and 7 trials for a random set of hyperparameters using the *QD-Grouped* model. This resulted in similarly stable results for all tested number of trials:\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr style=\"text-align: right;\">\n",
    "            <th></th>\n",
    "            <th>Accuracy Mean</th>\n",
    "            <th>Accuracy Variance</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>Number of Trials</th>\n",
    "            <th></th>\n",
    "            <th></th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr><th>3</th><td>56.75%</td><td>0.061</td></tr>\n",
    "        <tr><th>5</th><td>57.52%</td><td>0.050</td></tr>\n",
    "        <tr><th>7</th><td>56.59%</td><td>0.046</td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p align=\"center\" style=\"text-align: center;\">Figure 8. Number of trials comparison.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "Several of the models indicate a correlation between model prediction and presence of cognitive disease. The trained models are evaluated using only the left-out validation subset. To allow for a fair comparison across models, since sampling of subsets is done pseudo-randomly, results are also compared with what we describe as a naive classifier. The naive classifier is a simple model which labeling all inputs with the most common class; this metric corresponds to the class balance of the dataset so, for example, a subset where 45% of subjects are labeled as *healthy* would have a naive classifier accuracy of 55%.\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Accuracy</th>\n",
    "      <th>Î Naive Classifier</th>\n",
    "      <th>Area Under ROC</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Model</th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr><th>QD-Grouped*</th><td><b>67.60%</b></td><td><b>+10.45%</b></td><td><b>0.595</b></td></tr>\n",
    "    <tr><th>QD-Ungrouped</th><td>58.70%</td><td>+6.62%</td><td>0.571</td></tr>\n",
    "    <tr><th>QD-Transfer</th><td>50.69%</td><td>-1.39%</td><td>0.495</td></tr>\n",
    "    <tr><th>GoogLeNet-Transfer</th><td>56.19%</td><td>+4.49%</td><td>0.559</td></tr>\n",
    "    <tr><th>ResNet34-Transfer</th><td>56.35%</td><td>+4.70%</td><td>0.559</td></tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "<p align=\"center\" style=\"text-align: center;\">Figure 9. Model performance comparison.</p>\n",
    "\n",
    "#### Model Performance\n",
    "The model built using the grouped dataset significantly outperformed all other models, achieving an accuracy of >67.60% and an area under ROC of 0.595 using the optimal choice of parameters.\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr style=\"text-align: right;\">\n",
    "            <th>Parameter Name</th>\n",
    "            <th>Parameter Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr><td>Test Subset</td><td>20%</td></tr>\n",
    "        <tr><td>Validation Subset</td><td>20%</td></tr>\n",
    "        <tr><td>Kernel Size</td><td>9</td></tr>\n",
    "        <tr><td>Batch Size</td><td>48</td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<p align=\"center\" style=\"text-align: center;\">Figure 10. Optimal model training parameters.</p>\n",
    "\n",
    "The QD models consist of two convolution layers with max pooling, followed by two dense layers before the final softmax layer. This model, even though inspired by the prior work shown with *sketch-rnn*, lacks the recurrent component of that model architecture.\n",
    "\n",
    "<img src=\"../images/QD_model_architecture.png\">\n",
    "<p align=\"center\" style=\"text-align: center;\">Figure 11. QD model architecture.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "There are several key insights that were derived from our results. Most importantly, it appears to be plausible to perform early detection of cognitive diseases from subject drawings in a semi-automated way.\n",
    "\n",
    "Also, a custom model trained from scratch outperformed known models trained using transfer learning, even though conditions seemed favourable for transfer learning given the small dataset and multi-dimensional features. Further, the models trained using transfer learning barely outperformed the naive classifier and some had an area under ROC < 0.5 (see fig. 9).\n",
    "\n",
    "Another surprising result was that exclusion of any drawing category had a significant negative effect on the model performance, especially for the QD-grouped model which yielded a decrease in accuracy of over 5% leaving all other parameters equal. From our understanding of the data, we posit that this result is due to the increased information for each evaluation being available simultaneously to the model in a single datapoint. Alone, some drawing categories may have more relevant information than others; but using all drawing categories consistently yields better-performing models.\n",
    "\n",
    "This also explains why the QD-grouped model vastly outperformed all other models. Even though the much larger ungrouped dataset available during training should have a significant impact in model performance, grouping the drawings by evaluation allows for the model to combine the information from multiple drawings to achieve a better accuracy.\n",
    "\n",
    "#### Future Work\n",
    "One of the main areas left to explore are different parameters and model architectures. A completely exhaustive approach is not feasible due to the large amount of resources required for model training, but given that a relatively small amount of model architectures were tried, it is likely that further exploration would yield improved results.\n",
    "\n",
    "It may also be worth exploring why the transfer learning models did not perform well with this dataset, when other models did. It is possible that the larger number of model parameters requires more than 1000 iterations until convergence. \n",
    "\n",
    "For future experiments, changing the medium from paper to a digital drawing would potentially allow for a fully automated solution, avoiding most of the preprocessing currently necessary when using paper scans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1]D. Ha and D. Eck, âA Neural Representation of Sketch Drawings,â in ICLR 2018, 2018.\n",
    "\n",
    "[2]S. E. OâBryant et al., âA Serum ProteinâBased Algorithm for the Detection of Alzheimer Disease,â Arch Neurol, vol. 67, no. 9, pp. 1077â1081, Sep. 2010.\n",
    "\n",
    "[3]M. Talo, U. B. Baloglu, Ã. YÄ±ldÄ±rÄ±m, and U. Rajendra Acharya, âApplication of deep transfer learning for automated brain abnormality classification using MR images,â Cognitive Systems Research, vol. 54, pp. 176â188, May 2019.\n",
    "\n",
    "[4]K. A. Jellinger, B. Janetzky, J. Attems, and E. Kienzl, âBiomarkers for early diagnosis of Alzheimer disease: âALZheimer ASsociated geneââ a new blood biomarker?,â Journal of Cellular and Molecular Medicine, vol. 12, no. 4, pp. 1094â1117, 2008.\n",
    "\n",
    "[5]A. Rakhlin, A. Tiulpin, A. A. Shvets, A. A. Kalinin, V. I. Iglovikov, and S. Nikolenko, âBreast Tumor Cellularity Assessment Using Deep Neural Networks,â presented at the Proceedings of the IEEE International Conference on Computer Vision Workshops, 2019, pp. 0â0.\n",
    "\n",
    "[6]Z. ReitermanovÃ¡, âData Splitting,â 2010.\n",
    "\n",
    "[7]K. He, X. Zhang, S. Ren, and J. Sun, âDeep Residual Learning for Image Recognition,â presented at the Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770â778.\n",
    "\n",
    "[8]X. Shen, A. A. Efros, and M. Aubry, âDiscovering Visual Patterns in Art Collections with Spatially-consistent Feature Learning,â arXiv:1903.02678 [cs], Mar. 2019.\n",
    "\n",
    "[9]S. M. Clare, âDrawing Rules: The Importance of the Whole Brain for Learning Realistic Drawing,â Studies in Art Education, vol. 24, no. 2, pp. 126â130, Jan. 1983.\n",
    "\n",
    "[10]O. Almkvist and B. Winblad, âEarly diagnosis of Alzheimer dementia  based on clinical and biological factors,â European Archives of Psychiatry and Clinical Neurosciences, vol. 249, no. 3, pp. S3âS9, Dec. 1999.\n",
    "\n",
    "[11]L. Khedher, J. RamÃ­rez, J. M. GÃ³rriz, A. Brahim, and F. Segovia, âEarly diagnosis of Alzheimer×³s disease based on partial least squares, principal component analysis and support vector machine using segmented MRI images,â Neurocomputing, vol. 151, pp. 139â150, Mar. 2015.\n",
    "\n",
    "[12]L. Balagourouchetty, J. K. Pragatheeswaran, B. Pottakkat, and R. G, âGoogLeNet based Ensemble FCNet Classifier for Focal Liver Lesion Diagnosis,â IEEE Journal of Biomedical and Health Informatics, pp. 1â1, 2019.\n",
    "\n",
    "[13]R. Szeliski, âImage processing,â in Computer Vision: Algorithms and Applications, R. Szeliski, Ed. London: Springer London, 2011, pp. 87â180.\n",
    "\n",
    "[14]O. Russakovsky et al., âImageNet Large Scale Visual Recognition Challenge,â Int J Comput Vis, vol. 115, no. 3, pp. 211â252, Dec. 2015.\n",
    "\n",
    "[15]A. B. Jung et al., imgaug. 2019.\n",
    "\n",
    "[16]A. Buades, B. Coll, and J.-M. Morel, âNon-Local Means Denoising,â Image Processing On Line, vol. 1, pp. 208â212, Sep. 2011.\n",
    "\n",
    "[17]D. Soekhoe, P. van der Putten, and A. Plaat, âOn the Impact of Data Set Size in Transfer Learning Using Deep Neural Networks,â in Advances in Intelligent Data Analysis XV, 2016, pp. 50â60.\n",
    "\n",
    "[18]P. Ballester and R. M. Araujo, âOn the Performance of GoogLeNet and AlexNet Applied to Sketches,â in Thirtieth AAAI Conference on Artificial Intelligence, 2016.\n",
    "\n",
    "[19]M. Brantjes and A. Bouma, âQualitative analysis of the drawings of alzheimer patients,â Clinical Neuropsychologist, vol. 5, no. 1, pp. 41â52, Jan. 1991.\n",
    "\n",
    "[20]I. Rouleau, D. P. Salmon, N. Butters, C. Kennedy, and K. McGuire, âQuantitative and qualitative analyses of clock drawings in Alzheimerâs and Huntingtonâs disease,â Brain and Cognition, vol. 18, no. 1, pp. 70â87, Jan. 1992.\n",
    "\n",
    "[21]G. P. WolfâKlein, F. A. Silverstone, A. P. Levy, M. S. Brod, and J. Breuer, âScreening for Alzheimerâs Disease by Clock Drawing,â Journal of the American Geriatrics Society, vol. 37, no. 8, pp. 730â734, 1989.\n",
    "\n",
    "[22]M. Sert and E. BoyacÄ±, âSketch recognition using transfer learning,â Multimed Tools Appl, vol. 78, no. 12, pp. 17095â17112, Jun. 2019.\n",
    "\n",
    "[23]N. Japkowicz and S. Stephen, âThe class imbalance problem: A systematic study,â Intelligent Data Analysis, vol. 6, no. 5, pp. 429â449, Jan. 2002.\n",
    "\n",
    "[24]B. Agrell and O. Dehlin, âThe clock-drawing test,â Age and ageing, vol. 27, no. 3, pp. 399â403, 1998.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
