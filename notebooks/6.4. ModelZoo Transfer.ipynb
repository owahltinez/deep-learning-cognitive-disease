{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import models as torchvision_models\n",
    "\n",
    "from bananas.training.criteria import HaltCriteria\n",
    "from bananas.dataset import DataSet, DataType, Feature\n",
    "\n",
    "# Root path of project relative to this notebook\n",
    "ROOT = Path('..')\n",
    "\n",
    "sys.path.insert(1, str(ROOT / 'scripts'))\n",
    "from datamodels import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained models to be used as starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet34_model = torchvision_models.resnet34(pretrained=True)\n",
    "googlenet_model = torchvision_models.googlenet(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read subject data from local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(ROOT / 'datasets' / 'subject_diagnosis.csv', index_col=0)\n",
    "\n",
    "# Convert non-primitive fields\n",
    "df['processed_path'] = df['processed_path'].apply(lambda x: Path(x))\n",
    "df['image_path'] = df['image_path'].apply(lambda x: Path(x))\n",
    "df['template_path'] = df['template_path'].apply(lambda x: Path(x))\n",
    "df['drawing_box'] = df['drawing_box'].apply(lambda x: Box.load(x))\n",
    "df['template_box'] = df['template_box'].apply(lambda x: Box.load(x))\n",
    "\n",
    "# Remove all unnecessary columns from our dataset\n",
    "feat_keys = ['processed_path']\n",
    "group_columns = ['diagnosis']\n",
    "df = df[group_columns + feat_keys].copy()\n",
    "\n",
    "# Normalize all feature columns\n",
    "df = df.dropna()\n",
    "for col in feat_keys:\n",
    "    df[col] = df[col].apply(lambda x: str(ROOT / x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a file to hold intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = ROOT / 'results' / 'modelzoo_transfer.csv'\n",
    "\n",
    "if not results_file.exists():\n",
    "    pd.DataFrame(columns=[\n",
    "        'Key',\n",
    "        'Trial',\n",
    "        'Model',\n",
    "        'Subset splits', \n",
    "        'Batch size', \n",
    "        'Random seed', \n",
    "        'Δ naive classifier',\n",
    "        'Accuracy',\n",
    "        'Precision',\n",
    "        'Recall',\n",
    "        'Area under ROC']).set_index('Key').to_csv(results_file)\n",
    "\n",
    "trial_results = pd.read_csv(results_file, index_col='Key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train transfer learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, combinations\n",
    "    \n",
    "# Define all possible hyperparameters\n",
    "models = [resnet34_model, googlenet_model]\n",
    "batch_sizes = [24, 32, 48, 56, 64]\n",
    "test_splits = [.2, .25]\n",
    "validation_splits = [.2, .25]\n",
    "\n",
    "# Initialize random number generator without seed to randomize hyperparamters\n",
    "rnd = np.random.RandomState()\n",
    "\n",
    "# Cross product all hyperparameters\n",
    "parameter_combinations = list(product(\n",
    "    models, batch_sizes, test_splits, validation_splits))\n",
    "rnd.shuffle(parameter_combinations)\n",
    "\n",
    "target_label = 'SANO'\n",
    "target_column = 'diagnosis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bananas.core.mixins import HighDimensionalMixin\n",
    "from bananas.sampling.cross_validation import DataSplit\n",
    "from bananas.statistics import scoring\n",
    "from bananas.statistics.scoring import ScoringFunction\n",
    "from coconuts.learners.transfer_learning import TransferLearningModel, BaseNNClassifier\n",
    "\n",
    "# Perform 3 trials per parameter set to later compute the average\n",
    "trial_count = 3\n",
    "\n",
    "for model, batch_size, test_split, validation_split in tqdm(parameter_combinations, leave=False):\n",
    "\n",
    "    trial_params = {\n",
    "        'Model': model.__class__.__name__,\n",
    "        'Subset splits': (test_split, validation_split),\n",
    "        'Batch size': batch_size,\n",
    "    }\n",
    "    \n",
    "    # Execute the independent trials\n",
    "    for trial_num in tqdm(range(trial_count), leave=False, desc='Trial'):\n",
    "        trial_params['Trial'] = trial_num\n",
    "        \n",
    "        # If these parameters have already been tried, we can skip trial\n",
    "        trial_key = '|'.join(['%s=%s' % (k, str(v)) for k, v in trial_params.items()])\n",
    "        if trial_key in trial_results.index: continue\n",
    "            \n",
    "        # Create a single feature containing all image data\n",
    "        image_loader = ImageAugmenterLoader(\n",
    "            df['processed_path'].values,\n",
    "            resize=(3, 224, 224),\n",
    "            normalize=True,\n",
    "            convert='RGB')\n",
    "        features = [Feature(\n",
    "            image_loader,\n",
    "            kind=DataType.HIGH_DIMENSIOAL,\n",
    "            sample_size=10,\n",
    "            random_seed=0)]\n",
    "\n",
    "        # Define target feature\n",
    "        target_feature = Feature(\n",
    "            (df[target_column] == target_label).values, random_seed=0)\n",
    "\n",
    "        # Always re-initialize the randopm seed\n",
    "        random_seed = trial_num\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # Change seed at every iteration to ensure different dataset split\n",
    "            random_seed = np.random.RandomState(seed=random_seed).randint(1E6)\n",
    "            \n",
    "            # Build dataset, making sure that we have a left-out validation subset\n",
    "            dataset = DataSet(\n",
    "                features,\n",
    "                name=target_label,\n",
    "                target=target_feature,\n",
    "                random_seed=random_seed,\n",
    "                batch_size=batch_size,\n",
    "                test_split=test_split,\n",
    "                validation_split=validation_split)\n",
    "\n",
    "            # Compute test class balance to tell what minimum accuracy we should beat\n",
    "            test_idx = dataset.sampler.subsamplers[DataSplit.VALIDATION].data\n",
    "            test_classes = target_feature[test_idx]\n",
    "            test_class_balance = sum(test_classes) / len(test_classes)\n",
    "\n",
    "            # Rebuild dataset unless test class balance is within 5% of ground truth\n",
    "            true_class_balance = sum(target_feature[:] / len(target_feature))\n",
    "            if abs(test_class_balance - true_class_balance) < .05: break\n",
    "\n",
    "        # Instantiate learner using pre-trained model\n",
    "        learner = TransferLearningModel(\n",
    "            model,\n",
    "            freeze_base_model=True,\n",
    "            scoring_function=ScoringFunction.ACCURACY) \\\n",
    "            .apply_mixin(BaseNNClassifier, HighDimensionalMixin)\n",
    "\n",
    "        # Train learner using train dataset\n",
    "        learner.train(dataset.input_fn, progress=True, max_steps=1000)\n",
    "\n",
    "        # Test learner predictions using left-out dataset\n",
    "        # We have to do it one datapoint at a time instead of in batch to prevent overflow\n",
    "        yl, ylt = [], []\n",
    "        for i in tqdm(test_idx, leave=False):\n",
    "            X, y = dataset[i:i+1]\n",
    "            y = learner.label_encoder_.transform(y)\n",
    "            y_ = learner.predict_proba(X)\n",
    "            yl.append(y[0])\n",
    "            ylt.append(y_[0])\n",
    "        y, y_ = yl, ylt\n",
    "        score_auroc = scoring.score_auroc(y, y_)\n",
    "        score_accuracy = scoring.score_accuracy(y, y_)\n",
    "        score_precision = scoring.score_precision(y, y_)\n",
    "        score_recall = scoring.score_recall(y, y_)\n",
    "\n",
    "        # Store trial results\n",
    "        naive_accuracy = max(test_class_balance, 1 - test_class_balance)\n",
    "        trial_results.loc[trial_key] = {\n",
    "            **trial_params,\n",
    "            'Random seed': random_seed,\n",
    "            'Δ Naive Classifier': score_accuracy - naive_accuracy,\n",
    "            'Accuracy': score_accuracy,\n",
    "            'Precision': score_precision,\n",
    "            'Recall': score_recall,\n",
    "            'Area under ROC': score_auroc,\n",
    "        }\n",
    "        trial_results = trial_results.sort_values('Accuracy', ascending=False)\n",
    "        trial_results.to_csv(results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_results.head(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
